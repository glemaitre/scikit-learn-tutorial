{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution for Exercise 03\n",
    "\n",
    "The goal of this exercise is to evaluate the impact of feature\n",
    "preprocessing on a pipeline that uses a  decision-tree-based\n",
    "classifier instead of logistic regression.\n",
    "\n",
    "- The first question is to empirically evaluate whether scaling\n",
    "  numerical feature is helpful or not;\n",
    "\n",
    "- The second question is to evaluate whether it is empirically\n",
    "  better (both from a computational and a statistical perspective)\n",
    "  to use integer coded or one-hot encoded categories.\n",
    "\n",
    "Hint: `HistGradientBoostingClassifier` does not yet support\n",
    "sparse input data. You might want to use\n",
    "`OneHotEncoder(handle_unknown=\"ignore\", sparse=False)` to force the\n",
    "use a dense representation as a workaround."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://www.openml.org/data/get_csv/1595261/adult-census.csv\")\n",
    "\n",
    "# Or use the local copy: df = pd.read_csv('../datasets/adult-\n",
    "# census.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = df[target_name].to_numpy()\n",
    "data = df.drop(columns=[target_name, \"fnlwgt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [\n",
    "    c for c in data.columns if data[c].dtype.kind in [\"i\", \"f\"]]\n",
    "categorical_columns = [\n",
    "    c for c in data.columns if data[c].dtype.kind not in [\"i\", \"f\"]]\n",
    "\n",
    "categories = [\n",
    "    data[column].unique() for column in data[categorical_columns]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference pipeline (no numerical scaling and integer-coded categories)\n",
    "\n",
    "First let's time the pipeline we used in the main notebook to\n",
    "serve as a reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The different scores obtained are: \n",
      "[0.87224895 0.87143003 0.8745905  0.87346437 0.87796888]\n",
      "The accuracy is: 0.874 +- 0.002\n",
      "CPU times: user 2min 29s, sys: 4.82 s, total: 2min 34s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', OrdinalEncoder(categories=categories),\n",
    "     categorical_columns),], remainder=\"passthrough\")\n",
    "\n",
    "model = make_pipeline(preprocessor, HistGradientBoostingClassifier())\n",
    "scores = cross_val_score(model, data, target)\n",
    "print(f\"The different scores obtained are: \\n{scores}\")\n",
    "print(f\"The accuracy is: {scores.mean():.3f} +- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The different scores obtained are: \n",
      "[0.87224895 0.87143003 0.8745905  0.87346437 0.87796888]\n",
      "The accuracy is: 0.874 +- 0.002\n",
      "CPU times: user 2min 41s, sys: 5.62 s, total: 2min 47s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numerical', StandardScaler(), numerical_columns),\n",
    "    ('categorical', OrdinalEncoder(categories=categories),\n",
    "     categorical_columns),])\n",
    "\n",
    "model = make_pipeline(preprocessor, HistGradientBoostingClassifier())\n",
    "scores = cross_val_score(model, data, target)\n",
    "print(f\"The different scores obtained are: \\n{scores}\")\n",
    "print(f\"The accuracy is: {scores.mean():.3f} +- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "We can observe that both the accuracy and the training time are\n",
    "approximately the same as the reference pipeline (any time\n",
    "difference you might observe is not significant).\n",
    "\n",
    "Scaling numerical features is indeed useless for most decision\n",
    "tree models in general and for `HistGradientBoostingClassifier` in\n",
    "particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding of categorical variables\n",
    "\n",
    "For linear models, we have observed that integer coding of\n",
    "categorical variables can be very detrimental. However for\n",
    "`HistGradientBoostingClassifier` models, it does not seem to be the\n",
    "case as the cross-validation of the reference pipeline with\n",
    "`OrdinalEncoder` is good.\n",
    "\n",
    "Let's see if we can get an even better accuracy with\n",
    "`OneHotEncoding`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The different scores obtained are: \n",
      "[0.87286314 0.87173713 0.87407862 0.87407862 0.87807125]\n",
      "The accuracy is: 0.874 +- 0.002\n",
      "CPU times: user 4min 12s, sys: 9.12 s, total: 4min 22s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical',\n",
    "     OneHotEncoder(handle_unknown=\"ignore\", sparse=False),\n",
    "     categorical_columns),], remainder=\"passthrough\")\n",
    "\n",
    "model = make_pipeline(preprocessor, HistGradientBoostingClassifier())\n",
    "scores = cross_val_score(model, data, target)\n",
    "print(f\"The different scores obtained are: \\n{scores}\")\n",
    "print(f\"The accuracy is: {scores.mean():.3f} +- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "From an accuracy point of view, the result is almost exactly the\n",
    "same. The reason is that `HistGradientBoostingClassifier` is\n",
    "expressive and robust enough to deal with misleading ordering of\n",
    "integer coded categories (which was not the case for linear\n",
    "models).\n",
    "\n",
    "However from a computation point of view, the training time is\n",
    "significantly longer: this is caused by the fact that\n",
    "`OneHotEncoder` generates approximately 10 times more features than\n",
    "`OrdinalEncoder`.\n",
    "\n",
    "Note that the current implementation\n",
    "`HistGradientBoostingClassifier` is still incomplete, and once\n",
    "sparse representation are handled correctly, training time might\n",
    "improve with such kinds of encodings.\n",
    "\n",
    "The main take away message is that arbitrary integer coding of\n",
    "categories is perfectly fine for `HistGradientBoostingClassifier`\n",
    "and yields fast training times."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "python_scripts//py:percent,notebooks//ipynb",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
