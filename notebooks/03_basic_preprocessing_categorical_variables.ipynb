{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add intro with objectives\n",
    "\n",
    "# ## [markdown]\n",
    "# Let's first load the data as we did in the previous notebook.\n",
    "# TODO add link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://www.openml.org/data/get_csv/1595261/adult-census.csv\")\n",
    "# Or use the local copy: df = pd.read_csv('../datasets/adult-\n",
    "# census.csv')\n",
    "\n",
    "target_name = \"class\"\n",
    "target = df[target_name].to_numpy()\n",
    "\n",
    "data = df.drop(columns=[target_name, \"fnlwgt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with categorical variables\n",
    "\n",
    "As we have seen in the previous section, a numerical variable is\n",
    "a continuous quantity represented by a real or integer number.\n",
    "Those variables can be naturally handled by machine learning\n",
    "algorithms that typically composed of a sequence of arithmetic\n",
    "instructions such as additions and multiplications.\n",
    "\n",
    "By opposition, categorical variables have discrete values\n",
    "typically represented by string labels taken in a finite list of\n",
    "possible choices. For instance, the variable `native-country` in\n",
    "our dataset is a categorical variable because it encodes the data\n",
    "using a finite list of possible countries (along with the `?`\n",
    "marker when this information is missing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"native-country\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the remainder of this section, we will present different\n",
    "strategies to encode categorical data into numerical data which can\n",
    "be used by a machine- learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    c for c in data.columns if data[c].dtype.kind not in [\"i\", \"f\"]]\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_categorical = data[categorical_columns]\n",
    "data_categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The datasets is composed of {data_categorical.shape[1]} features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Encoding ordinal categories\n",
    "\n",
    "The most intuitive strategy is to encode each category with a\n",
    "number. The `OrdinalEncoder` will transform the data in such\n",
    "manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "data_encoded = encoder.fit_transform(data_categorical)\n",
    "\n",
    "print(\n",
    "    f\"The dataset encoded contains {data_encoded.shape[1]} features\")\n",
    "data_encoded[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all categories have been encoded for each feature\n",
    "independently. We can also notice that the number of features\n",
    "before and after the encoding is the same.\n",
    "\n",
    "However, one has to be careful when using this encoding strategy.\n",
    "Using this integer representation can lead the downstream models to\n",
    "make the assumption that the categories are ordered: 0 is smaller\n",
    "than 1 which is smaller than 2, etc.\n",
    "\n",
    "By default, `OrdinalEncoder` uses a lexicographical strategy to\n",
    "map string category labels to integers. This strategy is completely\n",
    "arbitrary and often be meaningless. For instance suppose the\n",
    "dataset has a categorical variable named \"size\" with categories\n",
    "such as \"S\", \"M\", \"L\", \"XL\". We would like the integer\n",
    "representation to respect the meaning of the sizes by mapping them\n",
    "to increasing integers such as 0, 1, 2, 3. However lexicographical\n",
    "strategy used by default would map the labels \"S\", \"M\", \"L\", \"XL\"\n",
    "to 2, 1, 0, 3.\n",
    "\n",
    "The `OrdinalEncoder` class accepts a \"categories\" constructor\n",
    "argument to pass an the correct ordering explicitly.\n",
    "\n",
    "If a categorical variable does not carry any meaningful order\n",
    "information then this encoding might be misleading to downstream\n",
    "statistical models and you might consider using one-hot encoding\n",
    "instead (see below).\n",
    "\n",
    "Note however that the impact a violation of this ordering\n",
    "assumption is really dependent on the downstream models (for\n",
    "instance linear models are much more sensitive than models built\n",
    "from a ensemble of decision trees).\n",
    "\n",
    "### Encoding nominal categories (without assuming any order)\n",
    "\n",
    "`OneHotEncoder` is an alternative encoder that can prevent the\n",
    "dowstream models to make a false assumption about the ordering of\n",
    "categories. For a given feature, it will create as many new columns\n",
    "as there are possible categories. For a given sample, the value of\n",
    "the column corresponding to the category will be set to `1` while\n",
    "all the columns of the other categories will be set to `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The dataset is composed of {data_categorical.shape[1]} features\"\n",
    ")\n",
    "data_categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "data_encoded = encoder.fit_transform(data_categorical)\n",
    "print(\n",
    "    f\"The dataset encoded contains {data_encoded.shape[1]} features\")\n",
    "data_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap this numpy array in a dataframe with informative\n",
    "column names as provided by the encoder object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_encoded = encoder.get_feature_names(data_categorical.columns)\n",
    "pd.DataFrame(data_encoded, columns=columns_encoded).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at how the workclass variable of the first 3 records has\n",
    "been encoded and compare this to the original string\n",
    "representation.\n",
    "\n",
    "The number of features after the encoding is than 10 times larger\n",
    "than in the original data because some variables such as\n",
    "`occupation` and `native- country` have many possible categories.\n",
    "\n",
    "We can now integrate this encoder inside a machine learning\n",
    "pipeline as in the case with numerical data: let's train a linear\n",
    "classifier on the encoded data and check the performance of this\n",
    "machine learning pipeline using cross- validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = make_pipeline(\n",
    "    OneHotEncoder(handle_unknown='ignore'),\n",
    "    LogisticRegression(solver='lbfgs', max_iter=1000))\n",
    "scores = cross_val_score(model, data_categorical, target)\n",
    "print(f\"The different scores obtained are: \\n{scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy is: {scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this representation of the categorical variables\n",
    "of the data is slightly more predictive of the revenue than the\n",
    "numerical variables that we used previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Exercise 1:\n",
    "\n",
    "- Try to fit a logistic regression model on categorical data\n",
    "  transformed by the OrdinalEncoder instead. What do you observe?\n",
    "\n",
    "Use the dedicated notebook to do this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Using numerical and categorical variables together\n",
    "\n",
    "In the previous sections, we saw that we need to treat data\n",
    "specifically depending of their nature (i.e. numerical or\n",
    "categorical).\n",
    "\n",
    "Scikit-learn provides a `ColumnTransformer` class which will\n",
    "dispatch some specific columns to a specific transformer making it\n",
    "easy to fit a single predictive model on a dataset that combines\n",
    "both kinds of variables together (heterogeneously typed tabular\n",
    "data).\n",
    "\n",
    "We can first define the columns depending on their data type:\n",
    "* **binary encoding** will be applied to categorical columns\n",
    "  with only too possible values (e.g. sex=male or sex=female in\n",
    "  this example). Each binary categorical columns will be mapped to\n",
    "  one numerical columns with 0 or 1 values.\n",
    "* **one-hot encoding** will be applied to categorical columns\n",
    "  with more that two possible categories. This encoding will create\n",
    "  one additional column for each possible categorical value.\n",
    "* **numerical scaling** numerical features which will be\n",
    "  standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_encoding_columns = ['sex']\n",
    "one_hot_encoding_columns = [\n",
    "    'workclass', 'education', 'marital-status', 'occupation',\n",
    "    'relationship', 'race', 'native-country']\n",
    "scaling_columns = [\n",
    "    'age', 'education-num', 'hours-per-week', 'capital-gain',\n",
    "    'capital-loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our `ColumnTransfomer` by specifying a list of\n",
    "triplet (preprocessor name, transformer, columns). Finally, we can\n",
    "define a pipeline to stack this \"preprocessor\" with our classifier\n",
    "(logistic regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('binary-encoder', OrdinalEncoder(), binary_encoding_columns),\n",
    "    ('one-hot-encoder', OneHotEncoder(handle_unknown='ignore'),\n",
    "     one_hot_encoding_columns),\n",
    "    ('standard-scaler', StandardScaler(), scaling_columns)])\n",
    "model = make_pipeline(\n",
    "    preprocessor, LogisticRegression(solver='lbfgs', max_iter=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model is more complex than the previous models but\n",
    "still follows the same API:\n",
    "- the `fit` method is called to preprocess the data then train\n",
    "  the classifier;\n",
    "- the `predict` method can make predictions on new data;\n",
    "- the `score` method is used to predict on the test data and\n",
    "  compare the predictions to the expected test labels to compute\n",
    "  the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=42)\n",
    "model.fit(data_train, target_train)\n",
    "model.predict(data_test)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(data_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can also be cross-validated as usual (instead of using\n",
    "a single train-test split):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, data, target, cv=5)\n",
    "print(f\"The different scores obtained are: \\n{scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy is: {scores.mean():.3f} +- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compound model has a higher predictive accuracy than the two\n",
    "models that used numerical and categorical variables in isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a more powerful model\n",
    "\n",
    "Linear models are very nice because they are usually very cheap\n",
    "to train, small to deploy, fast to predict and give a good\n",
    "baseline.\n",
    "\n",
    "However it is often useful to check whether more complex models\n",
    "such as ensemble of decision trees can lead to higher predictive\n",
    "performance.\n",
    "\n",
    "In the following we try a scalable implementation of the Gradient\n",
    "Boosting Machine algorithm. For this class of models, we know that\n",
    "contrary to linear models, it is useless to scale the numerical\n",
    "features and furthermore it is both safe and significantly more\n",
    "computationally efficient use an arbitrary integer encoding for the\n",
    "categorical variable even if the ordering is arbitrary. Therefore\n",
    "we adapt the preprocessing pipeline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# For each categorical column, extract the list of all possible\n",
    "# categories in some arbitary order.\n",
    "categories = [\n",
    "    data[column].unique() for column in data[categorical_columns]]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', OrdinalEncoder(categories=categories),\n",
    "     categorical_columns)], remainder=\"passthrough\")\n",
    "\n",
    "model = make_pipeline(preprocessor, HistGradientBoostingClassifier())\n",
    "model.fit(data_train, target_train)\n",
    "print(model.score(data_test, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We can observe that we get significantly higher accuracies with\n",
    "the Gradient Boosting model. This is often what we observe whenever\n",
    "the dataset has a large number of samples and limited number of\n",
    "informative features (e.g. less than 1000) with a mix of numerical\n",
    "and categorical variables.\n",
    "\n",
    "This explains why Gradient Boosted Machines are very popular\n",
    "among datascience practitioners who work with tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "\n",
    "- Check that scaling the numerical features does not impact the\n",
    "  speed or accuracy of `HistGradientBoostingClassifier`\n",
    "- Check that one-hot encoding the categorical variable does not\n",
    "  improve the accuracy of `HistGradientBoostingClassifier` but slows\n",
    "  down the training.\n",
    "\n",
    "Use the dedicated notebook to do this exercise."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "python_scripts//py:percent,notebooks//ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
