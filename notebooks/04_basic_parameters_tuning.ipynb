{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to scikit-learn: basic model hyper-parameters tuning\n",
    "\n",
    "The process of learning a predictive model is driven by a set of internal\n",
    "parameters and a set of training data. These internal parameters are called\n",
    "hyper-parameters and are specific for each family of models. In addition, a\n",
    "specific set of parameters are optimal for a specific dataset and thus they\n",
    "need to be optimized.\n",
    "\n",
    "This notebook shows:\n",
    "* the influence of changing model parameters;\n",
    "* how to tune these hyper-parameters;\n",
    "* how to evaluate the model performance together with hyper-parameter\n",
    "  tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://www.openml.org/data/get_csv/1595261/adult-census.csv\")\n",
    "# Or use the local copy:\n",
    "# df = pd.read_csv(os.path.join(\"..\", \"datasets\", \"adult-census.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = df[target_name].to_numpy()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(columns=[target_name, \"fnlwgt\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is loaded, we split it into a training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the preprocessing pipeline to transform differently\n",
    "the numerical and categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categorical_columns = [\n",
    "    'workclass', 'education', 'marital-status', 'occupation',\n",
    "    'relationship', 'race', 'native-country', 'sex']\n",
    "\n",
    "categories = [data[column].unique()\n",
    "              for column in data[categorical_columns]]\n",
    "\n",
    "categorical_preprocessor = OrdinalEncoder(categories=categories)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [('cat-preprocessor', categorical_preprocessor, categorical_columns)],\n",
    "    remainder='passthrough', sparse_threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use a tree-based classifier (i.e. histogram gradient-boosting) to\n",
    "predict whether or not a person earns more than 50,000 dollars a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the moment this line is required to import HistGradientBoostingClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor, HistGradientBoostingClassifier(random_state=42))\n",
    "model.fit(df_train, target_train)\n",
    "print(f\"The accuracy score using a {model.__class__.__name__} is \"\n",
    "      f\"{model.score(df_test, target_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## The issue of finding the best model parameters\n",
    "\n",
    "In the previous example, we created an histogram gradient-boosting classifier\n",
    "using the default parameters by omitting to explicitely set these parameters.\n",
    "\n",
    "However, there is no reasons that this set of parameters are optimal for our\n",
    "dataset. For instance, fine-tuning the histogram gradient-boosting can be\n",
    "achieved by finding the best combination of the following parameters: (i)\n",
    "`learning_rate`, (ii) `min_samples_leaf`, and (iii) `max_leaf_nodes`.\n",
    "Nevertheless, finding this combination manually will be tedious. Indeed,\n",
    "there are relationship between these parameters which are difficult to find\n",
    "manually: increasing the depth of trees (increasing `max_samples_leaf`)\n",
    "should be associated with a lower learning-rate.\n",
    "\n",
    "Scikit-learn provides tools to explore and evaluate the parameters\n",
    "space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best model hyper-parameters via exhaustive parameters search\n",
    "\n",
    "Our goal is to find the best combination of the parameters stated above.\n",
    "\n",
    "In short, we will set these parameters with some defined values, train our\n",
    "model on some data, and evaluate the model performance on some left out data.\n",
    "Ideally, we will select the parameters leading to the optimal performance on\n",
    "the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to find the name of the parameters to be set. We use the\n",
    "method `get_params()` to get this information. For instance, for a single\n",
    "model like the `HistGradientBoostingClassifier`, we can get the list such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The hyper-parameters are for a histogram GBDT model are:\")\n",
    "for param_name in HistGradientBoostingClassifier().get_params().keys():\n",
    "    print(param_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model of interest is a `Pipeline`, i.e. a serie of transformers and\n",
    "a predictor, the name of the estimator will be added at the front of the\n",
    "parameter name with a double underscore (\"dunder\") in-between (e.g.\n",
    "`estimator__parameters`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The hyper-parameters are for the full-pipeline are:\")\n",
    "for param_name in model.get_params().keys():\n",
    "    print(param_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters that we want to set are:\n",
    "- `'histgradientboostingclassifier__learning_rate'`: this parameter will\n",
    "  control the ability of a new tree to correct the error of the previous\n",
    "  sequence of trees;\n",
    "- `'histgradientboostingclassifier__max_leaf_nodes'`: this parameter will\n",
    "  control the depth of each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises:\n",
    "\n",
    "Use the previously defined model (called `model`) and using two nested `for`\n",
    "loops, make a search of the best combinations of the `learning_rate` and\n",
    "`max_leaf_nodes` parameters. In this regard, you will need to train and test\n",
    "the model by setting the parameters. The evaluation of the model should be\n",
    "performed using `cross_val_score`. We can propose to define the following\n",
    "parameters search:\n",
    "- `learning_rate` for the values 0.01, 0.1, and 1;\n",
    "- `max_leaf_nodes` for the values 5, 25, 45."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually writting the two `for` loops, scikit-learn provides a\n",
    "class called `GridSearchCV` which implement the exhaustive search implemented\n",
    "during the exercise.\n",
    "\n",
    "Let see how to use the `GridSearchCV` estimator for doing such search.\n",
    "Since the grid-search will be costly, we will only explore the combination\n",
    "learning-rate and the maximum number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'histgradientboostingclassifier__learning_rate': (0.01, 0.1, 1),\n",
    "    'histgradientboostingclassifier__max_leaf_nodes': (5, 43, 63),\n",
    "}\n",
    "model_grid_search = GridSearchCV(model, param_grid=param_grid,\n",
    "                                 n_jobs=4, cv=5)\n",
    "model_grid_search.fit(df_train, target_train)\n",
    "print(\n",
    "    f\"The accuracy score using a {model_grid_search.__class__.__name__} is \"\n",
    "    f\"{model_grid_search.score(df_test, target_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GridSearchCV` estimator takes a `param_grid` parameter which defines\n",
    "all hyper-parameters and their associated values. The grid-search will be in\n",
    "charge of creating all possible combinations and test them.\n",
    "\n",
    "The number of combinations will be equal to the cardesian product of the\n",
    "number of values to explore for each parameter (e.g. in our example 3 x 3\n",
    "combinations). Thus, adding new parameters with their associated values to be\n",
    "explored become rapidly computationally expensive.\n",
    "\n",
    "Once the grid-search is fitted, it can be used as any other predictor by\n",
    "calling `predict` and `predict_proba`. Internally, it will use the model with\n",
    "the best parameters found during `fit`.\n",
    "\n",
    "Get predictions for the 5 first samples using the estimator with the best\n",
    "parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid_search.predict(df_test.iloc[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can know about these parameters by looking at the `best_params_`\n",
    "attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best set of parameters is: \"\n",
    "      f\"{model_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can inspect all results which are stored in the attribute\n",
    "`cv_results_` of the grid-search. We will filter some specific columns to\n",
    "from these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the parameter names\n",
    "column_results = [f\"param_{name}\"for name in param_grid.keys()]\n",
    "column_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n",
    "\n",
    "cv_results = pd.DataFrame(model_grid_search.cv_results_)\n",
    "cv_results = cv_results[column_results].sort_values(\n",
    "    \"mean_test_score\", ascending=False)\n",
    "cv_results = cv_results.rename(\n",
    "    columns={\"param_histgradientboostingclassifier__learning_rate\":\n",
    "             \"learning-rate\",\n",
    "             \"param_histgradientboostingclassifier__max_leaf_nodes\":\n",
    "             \"max leaf nodes\"})\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 2 parameters, we might want to visualize the grid-search as a\n",
    "heatmap. We need to transform our `cv_results` into a dataframe where the\n",
    "rows will correspond to the learning-rate values and the columns will\n",
    "correspond to the maximum number of leaf and the content of the dataframe\n",
    "will be the mean test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_cv_results = cv_results.pivot_table(\n",
    "    values=\"mean_test_score\",\n",
    "    index=[\"learning-rate\"], columns=[\"max leaf nodes\"])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import heatmap\n",
    "\n",
    "ax = heatmap(heatmap_cv_results, annot=True, cmap=\"YlGnBu\", vmin=0.7, vmax=0.9)\n",
    "# FIXME: temporary fix since matplotlib 3.1.1 broke seaborn heatmap. Remove\n",
    "# with matplotlib 3.2\n",
    "ax.invert_yaxis()\n",
    "_ = ax.set_ylim([0, heatmap_cv_results.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `GridSearchCV` estimator, the parameters need to be specified\n",
    "explicitely. We mentioned that exploring a large number of values for\n",
    "different parameters will be quickly untractable.\n",
    "\n",
    "Instead, we can randomly generate the parameter candidates. The\n",
    "`RandomSearchCV` allows for such stochastic search. It is used similarly to\n",
    "the `GridSearchCV` but the sampling distributions need to be specified\n",
    "instead of the parameter values. For instance, we will draw candidates using\n",
    "a log-uniform distribution also called reciprocal distribution. In addition,\n",
    "we will optimize 2 other parameters:\n",
    "- `max_iter`: it corresponds to the number of trees in the ensemble;\n",
    "- `min_samples_leaf`: it corresponds to the minimum number of samples\n",
    "  required in a leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "class reciprocal_int:\n",
    "    def __init__(self, a, b):\n",
    "        self._distribution = reciprocal(a, b)\n",
    "\n",
    "    def rvs(self, *args, **kwargs):\n",
    "        return self._distribution.rvs(*args, **kwargs).astype(int)\n",
    "\n",
    "\n",
    "param_distributions = {\n",
    "    'histgradientboostingclassifier__l2_regularization': reciprocal(1e-6, 1),\n",
    "    'histgradientboostingclassifier__learning_rate': reciprocal(0.001, 1),\n",
    "    'histgradientboostingclassifier__max_leaf_nodes': reciprocal_int(5, 63),\n",
    "    'histgradientboostingclassifier__min_samples_leaf': reciprocal_int(3, 40),\n",
    "}\n",
    "model_random_search = RandomizedSearchCV(\n",
    "    model, param_distributions=param_distributions, n_iter=10,\n",
    "    n_jobs=4, cv=5)\n",
    "model_random_search.fit(df_train, target_train)\n",
    "print(\n",
    "    f\"The accuracy score using a {model_random_search.__class__.__name__} is \"\n",
    "    f\"{model_random_search.score(df_test, target_test):.2f}\")\n",
    "print(\n",
    "    f\"The best set of parameters is: {model_random_search.best_params_}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the results using the attributes `cv_results` as we previously\n",
    "did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the parameter names\n",
    "column_results = [f\"param_{name}\"for name in param_distributions.keys()]\n",
    "column_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n",
    "\n",
    "cv_results = pd.DataFrame(model_random_search.cv_results_)\n",
    "cv_results = cv_results[column_results].sort_values(\n",
    "    \"mean_test_score\", ascending=False)\n",
    "cv_results = cv_results.rename(\n",
    "    columns={\"param_histgradientboostingclassifier__l2_regularization\":\n",
    "             \"l2 regularization\",\n",
    "             \"param_histgradientboostingclassifier__learning_rate\":\n",
    "             \"learning-rate\",\n",
    "             \"param_histgradientboostingclassifier__max_leaf_nodes\":\n",
    "             \"max leaf nodes\",\n",
    "             \"param_histgradientboostingclassifier__min_samples_leaf\":\n",
    "             \"min samples leaf\",\n",
    "             \"mean_test_score\": \"mean test accuracy\",\n",
    "             \"rank_test_score\": \"ranking\"})\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, a randomized grid-search is usually run with a large number of\n",
    "iterations. In order to avoid the computation cost and still make a decent\n",
    "analysis, we load the results obtained from a similar search with 200\n",
    "iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cv_results = pd.read_csv(\n",
    "    os.path.join(\n",
    "        \"..\", \"figures\", \"randomized_search_results.csv\"),\n",
    "    index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have more than 2 paramters in our grid-search, we cannot visualize the\n",
    "results using a heatmap. However, we can us a parallel coordinates plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.parallel_coordinates(\n",
    "    cv_results.drop(columns=[\"ranking\", \"std_test_score\"]),\n",
    "    color=\"mean test accuracy\",\n",
    "    dimensions=[\"learning-rate\", \"l2 regularization\",\n",
    "                \"max leaf nodes\", \"min samples leaf\",\n",
    "                \"mean test accuracy\"],\n",
    "    color_continuous_scale=px.colors.diverging.Tealrose,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parallel coordinates plot will display the values of the hyper-parameters\n",
    "on different columns while the performance metric is color coded. Thus, we\n",
    "are able to quickly inspect if there is a range of hyper-parameters which is\n",
    "working or not.\n",
    "\n",
    "You can select a subset of searches by selecting for instance a range of\n",
    "value in the mean test accuracy metric.\n",
    "\n",
    "For instance, we observe that a small learning-rate (< 0.1)\n",
    "is not a good choice since a lot of the blue line (i.e. low accuracy) are\n",
    "emerging from this range of low values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises:\n",
    "\n",
    "- Build a machine learning pipeline:\n",
    "      * preprocess the categorical columns using a `OneHotEncoder` and use\n",
    "        a `StandardScaler` to normalize the numerical data.\n",
    "      * use a `LogisticRegression` as a predictive model.\n",
    "- Make an hyper-parameters search using `RandomizedSearchCV` and tuning the\n",
    "  parameters:\n",
    "      * `C` with values ranging from 0.001 to 10. You can use a reciprocal\n",
    "        distribution (i.e. `scipy.stats.reciprocal`);\n",
    "      * `solver` with possible values being `\"liblinear\"` and `\"lbfgs\"`;\n",
    "      * `penalty` with possible values being `\"l2\"` and `\"l1\"`;\n",
    "      * `drop` with possible values being `None` or `\"first\"`.\n",
    "\n",
    "You might get some `FitFailedWarning` and try to explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining evaluation and hyper-parameters search\n",
    "\n",
    "Cross-validation was used for searching for the best model parameters. We\n",
    "previously evaluated model performance through cross-validation as well. If\n",
    "we would like to combine both aspects, we need to perform a \"nested\"\n",
    "cross-validation. The \"outer\" cross-validation is applied to assess the model\n",
    "while the \"inner\" cross-validation sets the hyper-parameters of the model on\n",
    "the data set provided by the \"outer\" cross-validation. In practice, it is\n",
    "equivalent to including, `GridSearchCV`, `RandomSearchCV`, or any\n",
    "`EstimatorCV` in a `cross_val_score` or `cross_validate` function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# recall the definition of our grid-search\n",
    "param_distributions = {\n",
    "    'histgradientboostingclassifier__max_iter': reciprocal_int(10, 50),\n",
    "    'histgradientboostingclassifier__learning_rate': reciprocal(0.01, 1),\n",
    "    'histgradientboostingclassifier__max_leaf_nodes': reciprocal_int(15, 35),\n",
    "    'histgradientboostingclassifier__min_samples_leaf': reciprocal_int(3, 15),\n",
    "}\n",
    "model_random_search = RandomizedSearchCV(\n",
    "    model, param_distributions=param_distributions, n_iter=10,\n",
    "    n_jobs=4, cv=5)\n",
    "score = cross_val_score(model_random_search, data, target, n_jobs=4, cv=5)\n",
    "print(\n",
    "    f\"The accuracy score is: {score.mean():.3f} +- {score.std():.3f}\"\n",
    ")\n",
    "print(f\"The different scores obtained are: \\n{score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that such training might involve a variation of the hyper-parameters\n",
    "of the model. When analyzing such model, you should not only look at the\n",
    "overall model performance but look at the hyper-parameters variations as\n",
    "well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we have:\n",
    "* manually tuned the hyper-parameters of a machine-learning pipeline;\n",
    "* automatically tuned the hyper-parameters of a machine-learning pipeline by\n",
    "  by exhaustively searching the best combination of parameters from a defined\n",
    "  grid;\n",
    "* automatically tuned the hyper-parameters of a machine-learning pipeline by\n",
    "  drawing values candidates from some predefined distributions;\n",
    "* integrate an hyper-parameters tuning within a cross-validation.\n",
    "\n",
    "Key ideas discussed:\n",
    "* a grid-search is a costly search and does scale with the number of\n",
    "  parameters to search;\n",
    "* a randomized-search will run with a fixed given budget;\n",
    "* when assessing the performance of a model, hyper-parameters search should\n",
    "  be computed on the training data or can be integrated within another\n",
    "  cross-validation scheme."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "python_scripts//py:percent,notebooks//ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
