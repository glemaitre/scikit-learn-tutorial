{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to scikit-learn\n",
    "\n",
    "## Basic preprocessing and model fitting\n",
    "\n",
    "In this notebook, we present how to build predictive models on\n",
    "tabular datasets.\n",
    "\n",
    "In particular we will highlight:\n",
    "* the difference between numerical and categorical variables;\n",
    "* the importance of scaling numerical variables;\n",
    "* typical ways to deal categorical variables;\n",
    "* train predictive models on different kinds of data;\n",
    "* evaluate the performance of a model via cross-validation.\n",
    "\n",
    "## Introducing the dataset\n",
    "\n",
    "To this aim, we will use data from the 1994 Census bureau\n",
    "database. The goal with this data is to regress wages from\n",
    "heterogeneous data such as age, employment, education, family\n",
    "information, etc.\n",
    "\n",
    "Let's first load the data located in the `datasets` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://www.openml.org/data/get_csv/1595261/adult-census.csv\")\n",
    "\n",
    "# Or use the local copy:\n",
    "# df = pd.read_csv('../datasets/adult-census.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first records of this data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable in our study will be the \"class\" column while\n",
    "we will use the other columns as input variables for our model.\n",
    "This target column divides the samples (also known as records) into\n",
    "two groups: high income (>50K) vs low income (<=50K). The resulting\n",
    "prediction problem is therefore a binary classification problem.\n",
    "\n",
    "For simplicity, we will ignore the \"fnlwgt\" (final weight) column\n",
    "that was crafted by the creators of the dataset when sampling the\n",
    "dataset to be representative of the full census database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = df[target_name].to_numpy()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(columns=[target_name, \"fnlwgt\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of samples and the number of features\n",
    "available in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The dataset contains {data.shape[0]} samples and {data.shape[1]} \"\n",
    "    \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with numerical data\n",
    "\n",
    "The numerical data is the most natural type of data used in\n",
    "machine learning and can (almost) directly be fed to predictive\n",
    "models. We can quickly have a look at such data by selecting the\n",
    "subset of columns from the original data.\n",
    "\n",
    "We will use this subset of data to fit a linear classification\n",
    "model to predict the income class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [\n",
    "    c for c in data.columns if data[c].dtype.kind in [\"i\", \"f\"]]\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric = data[numerical_columns]\n",
    "data_numeric.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a machine learning model, it is important to leave\n",
    "out a subset of the data which we can use later to evaluate the\n",
    "trained model. The data used to fit a model a called training data\n",
    "while the one used to assess a model are called testing data.\n",
    "\n",
    "Scikit-learn provides an helper function `train_test_split` which\n",
    "will split the dataset into a training and a testing set. It will\n",
    "ensure that the data are shuffled randomly before splitting the\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data_numeric, target, random_state=42)\n",
    "\n",
    "print(\n",
    "    f\"The training dataset contains {data_train.shape[0]} samples and \"\n",
    "    f\"{data_train.shape[1]} features\")\n",
    "print(\n",
    "    f\"The testing dataset contains {data_test.shape[0]} samples and \"\n",
    "    f\"{data_test.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We will build a linear classification model called \"Logistic\n",
    "Regression\". The `fit` method is called to train the model from the\n",
    "input and target data. Only the training data should be given for\n",
    "this purpose.\n",
    "\n",
    "In addition, when checking the time required to train the model\n",
    "and internally check the number of iterations done by the solver to\n",
    "find a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start\n",
    "\n",
    "print(f\"The model {model.__class__.__name__} was trained in \"\n",
    "      f\"{elapsed_time:.3f} seconds for {model.n_iter_} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ignore the convergence warning for now and instead let's\n",
    "try to use our model to make some predictions on the first three\n",
    "records of the held out test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted = model.predict(data_test)\n",
    "target_predicted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = data_test.copy()\n",
    "predictions['predicted-class'] = target_predicted\n",
    "predictions['expected-class'] = target_test\n",
    "predictions['correct'] = target_predicted == target_test\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantitatively evaluate our model, we can use the method\n",
    "`score`. It will compute the classification accuracy when dealing\n",
    "with a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The test accuracy using a {model.__class__.__name__} is \"\n",
    "      f\"{model.score(data_test, target_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is mathematically equivalent as computing the average number\n",
    "of time the model makes a correct prediction on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(target_test == target_predicted).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "- What would be the score of a model that always predicts\n",
    "  `'>50K'`?\n",
    "- What would be the score of a model that always predicts\n",
    "  `' <= 50K'`?\n",
    "- Is 81% or 82% accuracy a good score for this problem?\n",
    "\n",
    "Hint: You can compute the cross-validated of a\n",
    "[DummyClassifier](https://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators)\n",
    "the performance of such baselines.\n",
    "\n",
    "Use the dedicated notebook to do this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider the `ConvergenceWarning` message that was\n",
    "raised previously when calling the `fit` method to train our model.\n",
    "This warning informs us that our model stopped learning because it\n",
    "reached the maximum number of iterations allowed by the user. This\n",
    "could potentially be detrimental for the model accuracy. We can\n",
    "follow the (bad) advice given in the warning message and increase\n",
    "the maximum number of iterations allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='lbfgs', max_iter=50000)\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The accuracy using a {model.__class__.__name__} is \"\n",
    "    f\"{model.score(data_test, target_test):.3f} with a fitting time of \"\n",
    "    f\"{elapsed_time:.3f} seconds in {model.n_iter_} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe now a longer training time but not significant\n",
    "improvement in the predictive performance. Instead of increasing\n",
    "the number of iterations, we can try to help fit the model faster\n",
    "by scaling the data first. A range of preprocessing algorithms in\n",
    "scikit-learn allows to transform the input data before training a\n",
    "model. We can easily combine these sequential operation with a\n",
    "scikit-learn `Pipeline` which will chain the operations and can be\n",
    "used as any other classifier or regressor. The helper function\n",
    "`make_pipeline` will create a `Pipeline` by giving the successive\n",
    "transformations to perform.\n",
    "\n",
    "In our case, we will standardize the data and then train a new\n",
    "logistic regression model on that new version of the dataset set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_train_scaled = scaler.fit_transform(data_train)\n",
    "data_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scaled = pd.DataFrame(data_train_scaled,\n",
    "                                 columns=data_train.columns)\n",
    "data_train_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(StandardScaler(),\n",
    "                      LogisticRegression(solver='lbfgs'))\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The accuracy using a {model.__class__.__name__} is \"\n",
    "    f\"{model.score(data_test, target_test):.3f} with a fitting time of \"\n",
    "    f\"{elapsed_time:.3f} seconds in {model[-1].n_iter_} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training time and the number of iterations is\n",
    "much shorter while the predictive performance (accuracy) stays the\n",
    "same.\n",
    "\n",
    "In the previous example, we split the original data into a\n",
    "training set and a testing set. This strategy has several issues:\n",
    "in the setting where the amount of data is limited, the subset of\n",
    "data used to train or test will be small; and the splitting was\n",
    "done in a random manner and we have no information regarding the\n",
    "confidence of the results obtained.\n",
    "\n",
    "Instead, we can use what cross-validation. Cross-validation\n",
    "consists in repeating this random splitting into training and\n",
    "testing sets and aggregate the model performance. By repeating the\n",
    "experiment, one can get an estimate of the variability of the model\n",
    "performance.\n",
    "\n",
    "The function `cross_val_score` allows for such experimental\n",
    "protocol by giving the model, the data and the target. Since there\n",
    "exists several cross-validation strategies, `cross_val_score`\n",
    "takes a parameter `cv` which defines the splitting strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, data_numeric, target, cv=5)\n",
    "print(f\"The different scores obtained are: \\n{scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by computing the standard-deviation of the cross-\n",
    "validation scores we can get an idea of the uncertainty of our\n",
    "estimation of the predictive performance of the model: in the above\n",
    "results, only the first 2 decimals seem to be trustworthy. Using a\n",
    "single train / test split would not allow us to know anything about\n",
    "the level of uncertainty of the accuracy of the model.\n",
    "\n",
    "Setting `cv=5` created 5 distinct splits to get 5 variations for\n",
    "the training and testing sets. Each training set is used to fit one\n",
    "model which is then scored on the matching test set. This strategy\n",
    "is called K-fold cross-validation where `K` corresponds to the\n",
    "number of splits.\n",
    "\n",
    "The following matplotlib code helps visualize how the datasets is\n",
    "partitionned between train and test samples at each iteration of\n",
    "the cross- validation procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "\n",
    "def plot_cv_indices(cv, X, y, ax, lw=20):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    splits = list(cv.split(X=X, y=y))\n",
    "    n_splits = len(splits)\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (train, test) in enumerate(splits):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.zeros(shape=X.shape[0], dtype=np.int32)\n",
    "        indices[train] = 1\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits))\n",
    "    ax.set(yticks=np.arange(n_splits + 2) + .5,\n",
    "           yticklabels=yticklabels, xlabel='Sample index',\n",
    "           ylabel=\"CV iteration\", ylim=[n_splits + .2,\n",
    "                                        -.2], xlim=[0, 100])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some random data points\n",
    "n_points = 100\n",
    "X = np.random.randn(n_points, 10)\n",
    "y = np.random.randn(n_points)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cv = KFold(5)\n",
    "_ = plot_cv_indices(cv, X, y, ax)\n",
    "\n",
    "# TODO: add summary here"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "python_scripts//py:percent,notebooks//ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
